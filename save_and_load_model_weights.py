# -*- coding: utf-8 -*-
"""save_and_load_model_weights.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BHxjYdph4w1Mh4BABzyENuxX3esEzFH7
"""

from string import punctuation
from os import listdir
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

from keras.models import load_model

import nltk
nltk.download('stopwords')

from string import punctuation
from os import listdir
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

import re
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import seaborn as sns
from nltk.tokenize import word_tokenize
from sklearn.base import TransformerMixin

from keras.models import Sequential
from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from keras.losses import binary_crossentropy
from keras.optimizers import Adam

import os
from sklearn.preprocessing import MultiLabelBinarizer
from keras.layers import Input, Dense
from sklearn.metrics import jaccard_similarity_score
from sklearn.utils import class_weight

dev_data = pd.read_csv("dev_data_birt.csv")
train_data = pd.read_csv("train_data_birt.csv")
test_data = pd.read_csv("test_data_birt.csv")

!wget http://nlp.stanford.edu/data/glove.840B.300d.zip

!unzip glove.840B.300d.zip

# from google.colab import drive
# drive.mount('/content/drive')

def _load_words():
    E = {}
    vocab = []
    with open('glove.840B.300d.txt', 'r', encoding="utf8") as file:
        for i, line in enumerate(file):
            l = line.split(' ')
            if l[0].isalpha():
                v = [float(i) for i in l[1:]]
                E[l[0]] = np.array(v)
                vocab.append(l[0])
    return np.array(vocab), E

V,E=_load_words()

def _get_word(v,E,C):
    for i, emb in enumerate(E):
        if np.array_equal(emb, v):
            return V[i]
    return None

train_text = train_data['Tweet']
dev_text = dev_data['Tweet']
test_text = test_data['Tweet']

text=pd.concat([train_text, dev_text,test_text])

# create the tokenizer
tokenizer = Tokenizer()#(num_words=5000)
# fit the tokenizer on the documents
tokenizer.fit_on_texts(text)

dev_encoded_docs = tokenizer.texts_to_sequences(dev_data['Tweet'])
train_encoded_docs = tokenizer.texts_to_sequences(train_data['Tweet'])
test_encoded_docs = tokenizer.texts_to_sequences(test_data['Tweet'])

max_length=30
train_encoded_docs_padded = pad_sequences(train_encoded_docs, maxlen=max_length, padding='post')
test_encoded_docs_padded = pad_sequences(test_encoded_docs, maxlen=max_length, padding='post')
dev_encoded_docs_padded = pad_sequences(dev_encoded_docs, maxlen=max_length, padding='post')

total_docs=pd.concat([pd.DataFrame(train_encoded_docs_padded),pd.DataFrame(dev_encoded_docs_padded)])

categories = ['anger',  'disgust', 'fear','sadness','surprise','anticipation', 'trust','love','optimism','pessimism','joy']
labels=pd.concat([train_data[categories],dev_data[categories]])

word_index = tokenizer.word_index

embedding_matrix = np.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = E.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(len(word_index) + 1,
                            300,
                            weights=[embedding_matrix],
                            input_length=max_length,
                            trainable=True)

sequence_input = Input(shape=(max_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

#either of the embedding layer options can be used
model = Sequential()
#model.add(Embedding(vocab_size, 20, input_length=max_length))
model.add(embedding_layer)
#model.add(Dropout(0.1))
#model.add(Conv1D(filter_length, 5, padding='valid', activation='relu', strides=1))
model.add(Bidirectional(LSTM(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))
model.add(GlobalMaxPool1D())
model.add(Dropout(0.1))
#model.add(MaxPooling1D(3))
model.add(Dense(110))
model.add(Activation('relu'))
model.add(Dense(11))#num_classes
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])

callbacks = [
    ReduceLROnPlateau(),
    EarlyStopping(patience=4),
    ModelCheckpoint(filepath='model-lstm.h5', save_best_only=True)
]

history = model.fit(total_docs, labels,
                    #class_weight=class_weight,
                    epochs=3,
                    batch_size=70,
                    validation_split=0.12,
#                     validation_data=(x_dev,y_dev),
                    callbacks=callbacks)

model2=load_model('model-lstm.h5')

model4 = Sequential()
#model.add(Embedding(vocab_size, 20, input_length=max_length))
model4.add(embedding_layer)
#model.add(Dropout(0.1))
#model.add(Conv1D(filter_length, 5, padding='valid', activation='relu', strides=1))
# model4.add(Bidirectional(LSTM(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))
model4.add(model2.layers[1])
model4.add(GlobalMaxPool1D())
model4.add(Dropout(0.1))
#model.add(MaxPooling1D(3))
model4.add(Dense(110))
model4.add(Activation('relu'))
model4.add(Dense(11))#num_classes
model4.add(Activation('sigmoid'))
model4.layers[1].trainable=True
model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])



model4.fit(total_docs, labels,
                    #class_weight=class_weight,
                    epochs=3,
                    batch_size=70,
                    validation_split=0.12)

